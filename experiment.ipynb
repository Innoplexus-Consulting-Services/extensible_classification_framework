{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h1 align=\"center\">Extensible Classification Framework</h1>\n",
    "---\n",
    "Extensible Classsification framework is an engineering effort to make a well defined ensemble engine for the text classifcation task. This notebook is an usage guide for the first relese of Extensible framework.\n",
    "\n",
    "This documentation will cover following points:\n",
    "- Features of the implementation\n",
    "- Network Architecture\n",
    "- Installation and example dataset download\n",
    "- Usage directions \n",
    "    - Processing data\n",
    "    - Getting custom vectors from fasttext model\n",
    "    - Defining model config\n",
    "    - Running experiments\n",
    "- Modifying  Components\n",
    "    - For intermediate users\n",
    "    - For advanced users\n",
    "\n",
    "# Salient features\n",
    "1. TorchText based flexible preprocessing\n",
    "2. Inbuilt custom biomedical tokenizer\n",
    "3. Inbuilt custom vectorization for biomedical term\n",
    "4. Accepts pretrained fasttext, Glove and Word2Vec vectors\n",
    "5. Splinning a run that performes N experiemnts and gives out best model\n",
    "6. Random best configuration search and report generation\n",
    "7. Custom saving and loading module\n",
    "8. Only 3 step to run all experiments\n",
    "9. End to end customization for intermediate and advanced users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Architecture\n",
    "![](extensible_framework.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Figure : Showing essential components involved in Extensible Classification framework. It has following components. (All the paramters shown in **bold** can have multiple values, various network will be constructed at run time by using these options.)*\n",
    "\n",
    "Iterator : \n",
    "  \n",
    "    A Torchtext Iterator, can take variable batch size and vocab size\n",
    "Embeddings : \n",
    "  \n",
    "- Pretrained fasttext, Glove and Word2Vec vectors\n",
    "- Accepts Fasttext model for generating custom embeddings at run time\n",
    "\n",
    "Bi- LSTM with Attention mechnaism: \n",
    "  \n",
    "    Standard Implementation of  LSTM with Attention mechnaism in forward and backward direction. \n",
    "    It has folowing configurable paramters\n",
    "    - final class num\n",
    "    - rnn_n_layers\n",
    "    - hidden_size\n",
    "    - pretrained_weights\n",
    "    - dropout\n",
    "    \n",
    "Convolution Neural Network ([Convolutional Neural Networks for Sentence Classification](https://arxiv.org/abs/1408.5882) Yoon Kim et. al., Implmemntation) : \n",
    "    It has folowing configurable paramters\n",
    "    - final class num\n",
    "    - pretrained_weights\n",
    "    - dropout\n",
    "    - out_channel_num\n",
    "    - kernel_sizes\n",
    "    \n",
    "Merge Layer\n",
    "    It combines output from the LSTM and CNN layer and provide it to FFN. The Merge mode can be:\n",
    "    - Concatenate\n",
    "    - Substract\n",
    "    - Add\n",
    "    - Matmul\n",
    "    \n",
    "Feed Forward Layer : \n",
    "    It has folowing configurable paramters\n",
    "    - output class num\n",
    "    - ffn_activation\n",
    "    - ffn_dropout\n",
    "    - perceptron_per_layer\n",
    "    - final_output_classes\n",
    "    - Num layers\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installation\n",
    "Installation Including following steps\n",
    "- Clonning repository\n",
    "- Installation through pip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` bash\n",
    "1. clone https://gitlab.innoplexus.de/Innoplexus-Consulting-Services/extensibleFramework.git\n",
    "2. pip install -U extensibleFramework/dist/extensible_classification_framework-0.0.1-py3-none-any.whl\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usage Directions\n",
    "`extensible_classification_framework` can be run by folowing 3 steps:\n",
    " 1. Preprocess\n",
    " 2. Prepare Vectors\n",
    " 3. Run training and paramter selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Preprocess \n",
    "The input file must be having two column seperated by tab, comma, semicolo or colon. The first colmn should be a class in interger the second column should be text one at each line.\n",
    "```bash\n",
    "python preprocess.py --input_file example/data/crude/small_data.tsv --output_destination example/data/processed/ --sep \"tab\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Prepare Vectors\n",
    "\n",
    "This step is to call a fast text module prepared usiing gensim for vectorization on custom tokens. For this you need to have a pre-trained fasttext model.\n",
    "```bash\n",
    "python prepare_vectors.py --model_path example/fasttext_model/fastText.model --train_file example/data/processed/20190401-171735_train.json --test_file example/data/processed/20190401-171735_test.json --vector_output_file example/vectors/vectors.vec\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Run training and paramter selection\n",
    "Before running the below given step, setup parameters in the `config.py`. A sample config.py looks like as given below:\n",
    "\n",
    "```python\n",
    "class parameters:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Here you may set parameters to be set\n",
    "        \"\"\"\n",
    "        self.cnn_rnn_vocab_size = 0 #len(sentiment_vocab)\n",
    "        self.cnn_rnn_embed_dim = 100\n",
    "        self.cnn_rnn_class_num = [200] # configurable\n",
    "        self.cnn_out_channel_num = [24] # configurable\n",
    "        self.cnn_kernel_sizes =  [3,4,5] # configurable\n",
    "        self.rnn_n_layers = [1,2,3] # configurable\n",
    "        self.rnn_hidden_size =  [128] # configurable\n",
    "        self.use_pretrained_weights = True\n",
    "        self.cnn_rnn_weights =  \"\" # sentiment_vocab.vectors\n",
    "        self.cnn_rnn_weight_is_trainable =   False\n",
    "        self.dropout = [0.2] # configurable\n",
    "        self.batch_size = 32 #batch_size\n",
    "        self.merge_mode = \"CONCAT\"\n",
    "        self.ffn_activation =  \"Relu\"\n",
    "        self.ffn_final_output_classes =  2\n",
    "        self.ffn_perceptron_per_layer = [[100,50, 25]] # configurable\n",
    "        self.ffn_layer_wise_dropout = 0.2\n",
    "        self.learning_rate =  [0.2] # configurable\n",
    "        self.momentum = [0.9] # configurable\n",
    "        self.device = \"\" # device\n",
    "        .\n",
    "        .\n",
    "        .\n",
    "```\n",
    "---\n",
    "All the configurable paramter are well indicated, you can provide many configuration for this paramters and all combination will be explored at run time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "```bash\n",
    "python main.py --train_json example/data/processed/20190401-171735_train.json --test_json example/data/processed/20190401-171735_test.json --embeddigns example/vectors/vectors.vec --epochs 1 --max_token 1000 --device \"gpu\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modifying  Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. For Intermediate users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some changes that you might want to do is as given below. **(These changes do not require recompilation and reinstallation)**\n",
    "\n",
    "1. Changing performance matrix\n",
    "2. Changing learning rate decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Changing performance metrics\n",
    "The existing performance matrix is implemented in `main.py` as : \n",
    "\n",
    "```python\n",
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "    rounded_preds = torch.argmax(preds, dim=1)\n",
    "    correct = (rounded_preds == torch.argmax(y, dim=1)).float() #convert into float for division \n",
    "    acc = correct.sum()/len(correct)\n",
    "    return acc\n",
    "```\n",
    "This function is being called by `test_accuracy_calculator` and `train` funtion for to calculate accuracy. \n",
    "You may change this function to calculate F1, Recall and Precision and run your experimetns. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Changing learning rate decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning rate decay is implemented as: \n",
    "```python\n",
    "if (i != 0 and i%10 == 0 ):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = param_group['lr']/2\n",
    "    print(\" === New Learning rate : \", param_group['lr'], \" === \")\n",
    "```\n",
    "By default learning rate is halved every 10 epochs. You may change this as well\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. For Advance Users\n",
    "**These chnages require recompilation and reinstallation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Changing  something in the core layers \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any changes made in the package (inside path `extensibleFramework/extensibleFramework/`) requires recompiling and re-installation.\n",
    "For example I want to change the ontology file which helps in the custom tokenization metrics. The change the existing ontology file following chnages need to be done. \n",
    "1. Replace 'resources/ontology_for_tokenizer.list' with new ontology\n",
    "2. cd to root project folder `/extensibleFramework`\n",
    "3. Recompile with `python setup.py sdist bdist_wheel`\n",
    "4. New source will be generated at folder dist/ as `dist/extensible_classification_framework-0.0.1-py3-none-any.whl`\n",
    "5. Install the new source with pip `pip install -U dist/extensible_classification_framework-0.0.1-py3-none-any.whl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
