{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change Log\n",
    "---\n",
    "handelling grid search params and correcting SAL module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/data/extensibleFramework/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from extensibleFramework.src.models import convolution_neural_network\n",
    "from extensibleFramework.src.models import recurrent_nn_with_attention\n",
    "from extensibleFramework.src.models import feed_forward_network\n",
    "from extensibleFramework.src.models import extra_layers\n",
    "from extensibleFramework.src.utils import saving_and_loading\n",
    "from extensibleFramework.src.utils import grid_search\n",
    "from extensibleFramework.src.models import config\n",
    "import torch\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import chakin\n",
    "import matplotlib.pyplot as plt\n",
    "from torchtext import data\n",
    "import nltk\n",
    "import json\n",
    "from torchtext import vocab\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import random\n",
    "import re\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import sys\n",
    "import random\n",
    "import tarfile\n",
    "import urllib\n",
    "from torchtext import data\n",
    "import datetime\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_frame = pd.read_csv(\"../data/crude/labeledTrainData.tsv\",sep=\"\\t\")\n",
    "data_frame = data_frame[['review', 'sentiment']]\n",
    "data_block  = data_frame.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SEED = 1\n",
    "split = 0.80\n",
    "batch_size  =32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# random.shuffle(data_block)\n",
    "\n",
    "# train_file = open('../data/processed/train.json', 'w')\n",
    "# test_file = open('../data/processed/test.json', 'w')\n",
    "# for i in  range(0,int(len(data_block)*split)):\n",
    "#     train_file.write(str(json.dumps({'review' : data_block[i][0], 'label' : data_block[i][1]}))+\"\\n\")\n",
    "# for i in  range(int(len(data_block)*split),len(data_block)):\n",
    "#     test_file.write(str(json.dumps({'review' : data_block[i][0], 'label' : data_block[i][1]}))+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize(sentiments):\n",
    "    \n",
    "    return nltk.tokenize.word_tokenize(sentiments)\n",
    "# def pad_to_equal(x):\n",
    "#     if len(x) < 61:\n",
    "#         return x + ['<pad>' for i in range(0, 61 - len(x))]\n",
    "#     else:\n",
    "#         return x[:61]\n",
    "def to_categorical(x):\n",
    "    x = int(x)\n",
    "    if x == 1:\n",
    "        return [0,1]\n",
    "    if x == 0:\n",
    "        return [1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "REVIEW = data.Field(sequential=True , tokenize=tokenize, use_vocab = True, lower=True,batch_first=True)\n",
    "LABEL = data.Field(is_target=True,use_vocab = False, sequential=False, preprocessing = to_categorical)\n",
    "fields = {'review': ('review', REVIEW), 'label': ('label', LABEL)}\n",
    "train_data , test_data = data.TabularDataset.splits(\n",
    "                            path = '',\n",
    "                            train = '../data/processed/train.json',\n",
    "                            test = '../data/processed/test.json',\n",
    "                            format = 'json',\n",
    "                            fields = fields)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# chakin.search(lang='English')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# chakin.download(number=12, save_dir = \"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# !unzip glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vec = vocab.Vectors(name = \"../embedidngs/glove.6B.100d.txt\",cache = \"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "REVIEW.build_vocab(train_data, test_data, max_size=400000, vectors=vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentiment_vocab = REVIEW.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64171"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentiment_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class parameters:\n",
    "    def __init__(self):\n",
    "        self.cnn_rnn_vocab_size = len(sentiment_vocab)\n",
    "        self.cnn_rnn_embed_dim = 100\n",
    "        self.cnn_rnn_class_num = [50,100,200]\n",
    "        self.cnn_out_channel_num = [8,12]\n",
    "        self.cnn_kernel_sizes =  [3,4,5]\n",
    "        self.rnn_n_layers = [1,2,3]\n",
    "        self.rnn_hidden_size =  [128, 256]\n",
    "        self.use_pretrained_weights = True\n",
    "        self.cnn_rnn_weights =  sentiment_vocab.vectors\n",
    "        self.cnn_rnn_weight_is_trainable =   False\n",
    "        self.dropout = [0.2, 0.5]\n",
    "        self.batch_size = batch_size\n",
    "        self.merge_mode = \"CONCAT\"\n",
    "        self.ffn_activation =  \"Relu\"\n",
    "        self.ffn_final_output_classes =  2\n",
    "        self.ffn_perceptron_per_layer = [[50, 25], [100,50, 25],  [200,100,50, 25]]\n",
    "        self.ffn_layer_wise_dropout = 0.2\n",
    "        self.learning_rate =  [0.1,0.2]\n",
    "        self.momentum = [0.5,0.9]\n",
    "        self.device = device\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SP = grid_search.searchParameters(parameters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_iter, test_iter = data.Iterator.splits(\n",
    "            (train_data, test_data), sort_key=lambda x: len(x.review),\n",
    "            batch_sizes=(batch_size,batch_size), device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 47]) torch.Size([32, 2])\n"
     ]
    }
   ],
   "source": [
    "for batch in test_iter:\n",
    "    feature, target = batch.review, batch.label\n",
    "    print(feature.data.shape, target.data.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64171, 100])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_vocab.vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "class ensemble_model(nn.Module):\n",
    "    def __init__(self, config_object):\n",
    "        super(ensemble_model, self).__init__() \n",
    "        self.cnn = convolution_neural_network.cnn_text(config_object)\n",
    "        self.rnn_attention = recurrent_nn_with_attention.RNNAttentionModel(config_object)\n",
    "        self.merge_layer = extra_layers.MergeAndFlattern(config_object)\n",
    "        self.ffn = feed_forward_network.ffn(config_object)\n",
    "    def forward(self, x):\n",
    "        cnn_output = self.cnn(x)\n",
    "        rnnAttention_output = self.rnn_attention(x)\n",
    "        merge_layer_output = self.merge_layer(cnn_output,rnnAttention_output )\n",
    "        final_output = self.ffn(merge_layer_output)\n",
    "        \n",
    "#         print(\"cnn_output : \",cnn_output.shape)\n",
    "#         print(\"rnn_input : \",rnnAttention_output.shape)\n",
    "#         print(\"merge_layer_output : \", merge_layer_output.shape)\n",
    "#         print(\"final_output : \", final_output.shape)\n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for parma_set in SP.random_search(1):\n",
    "    configuration = config.Config(parma_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64171 100 200 12 [3, 4, 5] 0.2 100\n"
     ]
    }
   ],
   "source": [
    "for parma_set in SP.random_search(1):\n",
    "    configuration = config.Config(parma_set)\n",
    "    EM  = ensemble_model(configuration)\n",
    "    EM  = EM.to(device)\n",
    "#     print(json.dumps(configuration, default=lambda x: x.__dict__))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnn_input = torch.Tensor(np.random.randint(5, size=[8, 10])).long().to(device)\n",
    "rnn_input = torch.Tensor(np.random.random([8, 10])).long().to(device) #batch_size, input_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# EM(rnn_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "    rounded_preds = torch.argmax(preds, dim=1)\n",
    "    correct = (rounded_preds == torch.argmax(y, dim=1)).float() #convert into float for division \n",
    "    acc = correct.sum()/len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        feature, target = batch.review, batch.label\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(feature.to(device))            \n",
    "        loss = criterion(predictions.type(torch.FloatTensor), target.type(torch.FloatTensor))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        acc = binary_accuracy(predictions.type(torch.FloatTensor), target.type(torch.FloatTensor))\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "    return model, epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_accuracy_calculator(model, test_iterator):\n",
    "    epoch_acc = 0\n",
    "    for batch in test_iterator:\n",
    "        if batch.review.shape[0] ==  32:\n",
    "            feature, target = batch.review, batch.label\n",
    "            predictions = model(feature.to(device))            \n",
    "            acc = binary_accuracy(predictions.type(torch.FloatTensor), target.type(torch.FloatTensor))\n",
    "            epoch_acc += acc.item()\n",
    "    return  epoch_acc / len(test_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(EM.parameters(), lr=0.1,momentum=0.9)\n",
    "criterion = nn.MSELoss()\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs  = 100\n",
    "log_interval = 1\n",
    "loss = []\n",
    "accuracy = []\n",
    "test_accuracy = []\n",
    "for i in tqdm(range(epochs)):\n",
    "    if (i != 0 and i%10 == 0 ):\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = param_group['lr']/2\n",
    "        print (\" === New Learning rate : \", param_group['lr'], \" === \")\n",
    "    \n",
    "    model, epoch_loss, epoch_acc = train(EM, train_iter, optimizer, criterion)\n",
    "    \n",
    "    test_acc = test_accuracy_calculator(model, test_iter)\n",
    "    accuracy.append(epoch_acc)\n",
    "    loss.append(epoch_loss)\n",
    "    test_accuracy.append(test_acc)\n",
    "    print(epoch_acc,test_acc,epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SAL = saving_and_loading.objectManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> train/20190306-144431 train\n",
      "++++++++ Model dumped with following parameters ++++++++\n",
      "--------------------------------------------------------\n",
      "cnn.embed.weight \t torch.Size([64171, 100])\n",
      "cnn.convs1.0.weight \t torch.Size([8, 1, 3, 3])\n",
      "cnn.convs1.0.bias \t torch.Size([8])\n",
      "cnn.convs1.1.weight \t torch.Size([8, 1, 4, 4])\n",
      "cnn.convs1.1.bias \t torch.Size([8])\n",
      "cnn.convs1.2.weight \t torch.Size([8, 1, 5, 5])\n",
      "cnn.convs1.2.bias \t torch.Size([8])\n",
      "cnn.fc1.weight \t torch.Size([100, 24])\n",
      "cnn.fc1.bias \t torch.Size([100])\n",
      "rnn_attention.word_embeddings.weight \t torch.Size([64171, 100])\n",
      "rnn_attention.lstm.weight_ih_l0 \t torch.Size([1024, 100])\n",
      "rnn_attention.lstm.weight_hh_l0 \t torch.Size([1024, 256])\n",
      "rnn_attention.lstm.bias_ih_l0 \t torch.Size([1024])\n",
      "rnn_attention.lstm.bias_hh_l0 \t torch.Size([1024])\n",
      "rnn_attention.label.weight \t torch.Size([100, 256])\n",
      "rnn_attention.label.bias \t torch.Size([100])\n",
      "ffn.network.input.weight \t torch.Size([50, 200])\n",
      "ffn.network.input.bias \t torch.Size([50])\n",
      "ffn.network.Linear0.weight \t torch.Size([25, 50])\n",
      "ffn.network.Linear0.bias \t torch.Size([25])\n",
      "ffn.network.output.weight \t torch.Size([2, 25])\n",
      "ffn.network.output.bias \t torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "SAL.saver(EM, \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EM = SAL.loader(\"train/20190306-144431/ensemble_model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'collections.OrderedDict' object has no attribute 'state_dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-4cf549f22231>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mEM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'collections.OrderedDict' object has no attribute 'state_dict'"
     ]
    }
   ],
   "source": [
    "EM.state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
