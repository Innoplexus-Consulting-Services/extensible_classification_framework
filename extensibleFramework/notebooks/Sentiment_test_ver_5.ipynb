{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change Log\n",
    "---\n",
    "Checking saving and loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/data/extensibleFramework/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from extensibleFramework.src.models import convolution_neural_network\n",
    "from extensibleFramework.src.models import recurrent_nn_with_attention\n",
    "from extensibleFramework.src.models import feed_forward_network\n",
    "from extensibleFramework.src.models import extra_layers\n",
    "from extensibleFramework.src.utils import saving_and_loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import chakin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torchtext import data\n",
    "import nltk\n",
    "import json\n",
    "from torchtext import vocab\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import random\n",
    "import re\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import sys\n",
    "import random\n",
    "import tarfile\n",
    "import urllib\n",
    "from torchtext import data\n",
    "import datetime\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_frame = pd.read_csv(\"labeledTrainData.tsv\",sep=\"\\t\")\n",
    "data_frame = data_frame[['review', 'sentiment']]\n",
    "data_block  = data_frame.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SEED = 1\n",
    "split = 0.80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random.shuffle(data_block)\n",
    "\n",
    "# train_file = open('train.json', 'w')\n",
    "# test_file = open('test.json', 'w')\n",
    "# for i in  range(0,int(len(data_block)*split)):\n",
    "#     train_file.write(str(json.dumps({'review' : data_block[i][0], 'label' : data_block[i][1]}))+\"\\n\")\n",
    "# for i in  range(int(len(data_block)*split),len(data_block)):\n",
    "#     test_file.write(str(json.dumps({'review' : data_block[i][0], 'label' : data_block[i][1]}))+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize(sentiments):\n",
    "    \n",
    "    return nltk.tokenize.word_tokenize(sentiments)\n",
    "# def pad_to_equal(x):\n",
    "#     if len(x) < 61:\n",
    "#         return x + ['<pad>' for i in range(0, 61 - len(x))]\n",
    "#     else:\n",
    "#         return x[:61]\n",
    "def to_categorical(x):\n",
    "    x = int(x)\n",
    "    if x == 1:\n",
    "        return [0,1]\n",
    "    if x == 0:\n",
    "        return [1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SENTIMENT = data.Field(sequential=True , tokenize=tokenize, use_vocab = True, lower=True,batch_first=True)\n",
    "LABEL = data.Field(is_target=True,use_vocab = False, sequential=False, preprocessing = to_categorical)\n",
    "fields = {'sentiment': ('sentiment', SENTIMENT), 'label': ('label', LABEL)}\n",
    "train_data , test_data = data.TabularDataset.splits(\n",
    "                            path = '',\n",
    "                            train = 'train.json',\n",
    "                            test = 'test.json',\n",
    "                            format = 'json',\n",
    "                            fields = fields)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# chakin.search(lang='English')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# chakin.download(number=12, save_dir = \"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# !unzip glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vec = vocab.Vectors(name = \"glove.6B.100d.txt\",cache = \"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SENTIMENT.build_vocab(train_data, test_data, max_size=400000, vectors=vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_iter, test_iter = data.Iterator.splits(\n",
    "        (train_data, test_data), sort_key=lambda x: len(x.sentiment),\n",
    "        batch_sizes=(32,32), device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 46]) torch.Size([32, 2])\n"
     ]
    }
   ],
   "source": [
    "for batch in test_iter:\n",
    "    feature, target = batch.sentiment, batch.label\n",
    "    print(feature.data.shape, target.data.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentiment_vocab = SENTIMENT.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([118522, 100])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_vocab.vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "class ensemble_model(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, class_num, \\\n",
    "                 out_channel_num, kernel_size, dropout, input_size, hidden_size,\\\n",
    "                 batch_size, n_layers=1, rnn_unit=\"GRU\",weight=None, weight_is_trainable = False):\n",
    "        super(ensemble_model, self).__init__() \n",
    "        self.cnn = convolution_neural_network.cnn_text(vocab_size, embed_dim, class_num, out_channel_num, \\\n",
    "                                                       kernel_size, dropout,weight=None, weight_is_trainable = weight_is_trainable)\n",
    "        self.rnn_attention = recurrent_nn_with_attention.RNNAttentionModel(output_size=class_num, hidden_size=hidden_size,\\\n",
    "                                                                          vocab_size=vocab_size, embedding_length=embed_dim, \\\n",
    "                                                                          batch_size=batch_size, device = device,\\\n",
    "                                                                          weight=None, weight_is_trainable = weight_is_trainable)\n",
    "        self.merge_layer = extra_layers.MergeAndFlattern(mode=\"CONCAT\",batch_size=batch_size)\n",
    "        self.ffn = feed_forward_network.ffn(activation = \"Relu\", num_layer = 2 , input_size = 200 , output_size = 2, perceptron_per_layer = [100,50] , dropout = [0.2,0.2])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        cnn_output = self.cnn(x)\n",
    "        rnnAttention_output = self.rnn_attention(x)\n",
    "        merge_layer_output = self.merge_layer(cnn_output,rnnAttention_output )\n",
    "        final_output = self.ffn(merge_layer_output)\n",
    "        \n",
    "#         print(\"cnn_output : \",cnn_output.shape)\n",
    "#         print(\"rnn_input : \",rnnAttention_output.shape)\n",
    "#         print(\"merge_layer_output : \", merge_layer_output.shape)\n",
    "#         print(\"final_output : \", final_output.shape)\n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_size = 10\n",
    "hidden_size = 128\n",
    "batch_size = 32\n",
    "n_layers = 1\n",
    "vocab_size = len(SENTIMENT.vocab)\n",
    "embed_dim  = 100\n",
    "class_num =  100\n",
    "out_channel_num = 8\n",
    "kernel_sizes = [3,4,5]\n",
    "dropout = 0.2\n",
    "\n",
    "EM  = ensemble_model(vocab_size=vocab_size, embed_dim=embed_dim, class_num=class_num,out_channel_num=out_channel_num,\\\n",
    "                    kernel_size=kernel_sizes, dropout=dropout, input_size = input_size, hidden_size=hidden_size,\\\n",
    "                    batch_size = batch_size, n_layers = n_layers, rnn_unit = \"GRU\",weight=sentiment_vocab.vectors, \\\n",
    "                     weight_is_trainable = True)\n",
    "EM  = EM.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnn_input = torch.Tensor(np.random.randint(5, size=[8, 10])).long().to(device)\n",
    "rnn_input = torch.Tensor(np.random.random([8, 10])).long().to(device) #batch_size, input_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# EM(rnn_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "    rounded_preds = torch.argmax(preds, dim=1)\n",
    "    correct = (rounded_preds == torch.argmax(y, dim=1)).float() #convert into float for division \n",
    "    acc = correct.sum()/len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        feature, target = batch.sentiment, batch.label\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(feature.to(device))            \n",
    "        loss = criterion(predictions.type(torch.FloatTensor), target.type(torch.FloatTensor))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        acc = binary_accuracy(predictions.type(torch.FloatTensor), target.type(torch.FloatTensor))\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "    return model, epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_accuracy_calculator(model, test_iterator):\n",
    "    epoch_acc = 0\n",
    "    for batch in test_iterator:\n",
    "        if batch.sentiment.shape[0] ==  32:\n",
    "            feature, target = batch.sentiment, batch.label\n",
    "            predictions = model(feature.to(device))            \n",
    "            acc = binary_accuracy(predictions.type(torch.FloatTensor), target.type(torch.FloatTensor))\n",
    "            epoch_acc += acc.item()\n",
    "    return  epoch_acc / len(test_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(EM.parameters(), lr=0.1,momentum=0.9)\n",
    "criterion = nn.MSELoss()\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# epochs  = 100\n",
    "# log_interval = 1\n",
    "# loss = []\n",
    "# accuracy = []\n",
    "# test_accuracy = []\n",
    "# for i in tqdm(range(epochs)):\n",
    "#     model, epoch_loss, epoch_acc = train(EM, train_iter, optimizer, criterion)\n",
    "#     test_acc = test_accuracy_calculator(model, test_iter)\n",
    "#     accuracy.append(epoch_acc)\n",
    "#     loss.append(epoch_loss)\n",
    "#     test_accuracy.append(test_acc)\n",
    "#     print(epoch_acc,test_acc,epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SAL = saving_and_loading.objectManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++ Model dumped with following parameters ++++++++\n",
      "--------------------------------------------------------\n",
      "cnn.embed.weight \t torch.Size([118522, 100])\n",
      "cnn.convs1.0.weight \t torch.Size([8, 1, 3, 3])\n",
      "cnn.convs1.0.bias \t torch.Size([8])\n",
      "cnn.convs1.1.weight \t torch.Size([8, 1, 4, 4])\n",
      "cnn.convs1.1.bias \t torch.Size([8])\n",
      "cnn.convs1.2.weight \t torch.Size([8, 1, 5, 5])\n",
      "cnn.convs1.2.bias \t torch.Size([8])\n",
      "cnn.fc1.weight \t torch.Size([100, 24])\n",
      "cnn.fc1.bias \t torch.Size([100])\n",
      "rnn_attention.word_embeddings.weight \t torch.Size([118522, 100])\n",
      "rnn_attention.lstm.weight_ih_l0 \t torch.Size([512, 100])\n",
      "rnn_attention.lstm.weight_hh_l0 \t torch.Size([512, 128])\n",
      "rnn_attention.lstm.bias_ih_l0 \t torch.Size([512])\n",
      "rnn_attention.lstm.bias_hh_l0 \t torch.Size([512])\n",
      "rnn_attention.label.weight \t torch.Size([100, 128])\n",
      "rnn_attention.label.bias \t torch.Size([100])\n",
      "ffn.network.input.weight \t torch.Size([100, 200])\n",
      "ffn.network.input.bias \t torch.Size([100])\n",
      "ffn.network.Linear0.weight \t torch.Size([50, 100])\n",
      "ffn.network.Linear0.bias \t torch.Size([50])\n",
      "ffn.network.output.weight \t torch.Size([2, 50])\n",
      "ffn.network.output.bias \t torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "SAL.saver(EM, \"./EM.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EM = SAL.loader(\"./EM.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
