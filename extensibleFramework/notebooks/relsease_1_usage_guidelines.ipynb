{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extensible Framwork\n",
    "Extensible framework is an engineering effort to make a well defined ensemble engine for the text classifcation task.\n",
    "\n",
    "This notebook is an usage guide for the first relese of Extensible framwork."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This documentation will cover following points:\n",
    "1. Installation and example dataset download\n",
    "2. Preprocessing with Torchtext\n",
    "3. Defining model config\n",
    "4. Defining data iterator\n",
    "5. Definning emsemble model\n",
    "6. Defining Model  Training/Test Functions\n",
    "7. Defining optimizer, loss and learning rate\n",
    "8. Running the training for desired epochs\n",
    "9. Model Save ans load mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation and example data download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installtion support  in not planned in this release but you can always use it by adding the module to system Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/data/extensibleFramework/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from extensibleFramework.src.models import convolution_neural_network\n",
    "from extensibleFramework.src.models import recurrent_nn_with_attention\n",
    "from extensibleFramework.src.models import feed_forward_network\n",
    "from extensibleFramework.src.models import extra_layers\n",
    "from extensibleFramework.src.utils import saving_and_loading\n",
    "from extensibleFramework.src.models import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import chakin\n",
    "import matplotlib.pyplot as plt\n",
    "from torchtext import data\n",
    "import nltk\n",
    "import json\n",
    "from torchtext import vocab\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import random\n",
    "import re\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import sys\n",
    "import random\n",
    "import tarfile\n",
    "import urllib\n",
    "from torchtext import data\n",
    "import datetime\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_frame = pd.read_csv(\"labeledTrainData.tsv\",sep=\"\\t\")\n",
    "data_frame = data_frame[['review', 'sentiment']]\n",
    "data_block  = data_frame.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Splitting data in to train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "split = 0.80 # train test split\n",
    "random.shuffle(data_block)\n",
    "train_file = open('train.json', 'w')\n",
    "test_file = open('test.json', 'w')\n",
    "for i in  range(0,int(len(data_block)*split)):\n",
    "    train_file.write(str(json.dumps({'review' : data_block[i][0], 'label' : data_block[i][1]}))+\"\\n\")\n",
    "for i in  range(int(len(data_block)*split),len(data_block)):\n",
    "    test_file.write(str(json.dumps({'review' : data_block[i][0], 'label' : data_block[i][1]}))+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing with Torchtext\n",
    "Please trfer to Torchtext module for detailed usage : [http://torchtext.readthedocs.io/](http://torchtext.readthedocs.io/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize(sentiments):\n",
    "    return nltk.tokenize.word_tokenize(sentiments)\n",
    "def to_categorical(x):\n",
    "    x = int(x)\n",
    "    if x == 1:\n",
    "        return [0,1]\n",
    "    if x == 0:\n",
    "        return [1,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SENTIMENT = data.Field(sequential=True , tokenize=tokenize, use_vocab = True, lower=True,batch_first=True)\n",
    "LABEL = data.Field(is_target=True,use_vocab = False, sequential=False, preprocessing = to_categorical)\n",
    "fields = {'sentiment': ('sentiment', SENTIMENT), 'label': ('label', LABEL)}\n",
    "train_data , test_data = data.TabularDataset.splits(\n",
    "                            path = '',\n",
    "                            train = 'train.json',\n",
    "                            test = 'test.json',\n",
    "                            format = 'json',\n",
    "                            fields = fields)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# chakin.search(lang='English')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# chakin.download(number=12, save_dir = \"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# !unzip glove.6B.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Vocab on the basis of fasttext vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vec = vocab.Vectors(name = \"wiki.en.vec\",cache = \"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SENTIMENT.build_vocab(train_data, test_data, vectors=vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentiment_vocab = SENTIMENT.vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# defining config object \n",
    "cnn_rnn_vocab_size = len(SENTIMENT.vocab)\n",
    "cnn_rnn_embed_dim = 300\n",
    "cnn_rnn_class_num = 50\n",
    "out_channel_num = 8\n",
    "cnn_kernel_sizes = [3,4,5]\n",
    "rnn_n_layers = 1\n",
    "rnn_hidden_size = 128\n",
    "use_pretrained_weights = True\n",
    "cnn_rnn_weights = sentiment_vocab.vectors\n",
    "cnn_rnn_weight_is_trainable =  True\n",
    "dropout = 0.2\n",
    "batch_size = 32\n",
    "merge_mode = \"CONCAT\"\n",
    "ffn_activation = \"Relu\"\n",
    "ffn_final_output_classes = 2\n",
    "ffn_num_layer = 2 \n",
    "ffn_perceptron_per_layer = [50, 25]\n",
    "ffn_layer_wise_dropout = [0.2,0.2]\n",
    "device = device\n",
    "# constructing model config object\n",
    "config = config.Config(cnn_rnn_vocab_size, cnn_rnn_embed_dim, cnn_rnn_class_num, out_channel_num, \\\n",
    "        cnn_kernel_sizes, rnn_n_layers, rnn_hidden_size, use_pretrained_weights,cnn_rnn_weights, cnn_rnn_weight_is_trainable,\\\n",
    "            dropout, batch_size, merge_mode, ffn_activation, ffn_final_output_classes, ffn_num_layer, ffn_perceptron_per_layer,\\\n",
    "                       ffn_layer_wise_dropout, device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Data Iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_iter, test_iter = data.Iterator.splits(\n",
    "        (train_data, test_data), sort_key=lambda x: len(x.sentiment),\n",
    "        batch_sizes=(batch_size,batch_size), device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking Iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in test_iter:\n",
    "    feature, target = batch.sentiment, batch.label\n",
    "    print(feature.data.shape, target.data.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking total vocab size. With this vocabs we will initialize the embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_vocab.vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Defining Ensemble Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ensemble_model(nn.Module):\n",
    "    def __init__(self, config_object):\n",
    "        super(ensemble_model, self).__init__() \n",
    "        self.cnn = convolution_neural_network.cnn_text(config_object)\n",
    "        self.rnn_attention = recurrent_nn_with_attention.RNNAttentionModel(config_object)\n",
    "        self.merge_layer = extra_layers.MergeAndFlattern(config_object)\n",
    "        self.ffn = feed_forward_network.ffn(config_object)\n",
    "    def forward(self, x):\n",
    "        cnn_output = self.cnn(x)\n",
    "        rnnAttention_output = self.rnn_attention(x)\n",
    "        merge_layer_output = self.merge_layer(cnn_output,rnnAttention_output )\n",
    "        final_output = self.ffn(merge_layer_output)\n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EM  = ensemble_model(config)\n",
    "EM  = EM.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking model with example dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnn_input = torch.Tensor(np.random.randint(5, size=[8, 10])).long().to(device)\n",
    "rnn_input = torch.Tensor(np.random.random([8, 10])).long().to(device) #batch_size, input_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Model  Training/Test Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "    rounded_preds = torch.argmax(preds, dim=1)\n",
    "    correct = (rounded_preds == torch.argmax(y, dim=1)).float() #convert into float for division \n",
    "    acc = correct.sum()/len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    \"\"\"\n",
    "    To iterate over given dataset for one epoch\n",
    "    \"\"\"\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        feature, target = batch.sentiment, batch.label\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(feature.to(device))            \n",
    "        loss = criterion(predictions.type(torch.FloatTensor), target.type(torch.FloatTensor))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        acc = binary_accuracy(predictions.type(torch.FloatTensor), target.type(torch.FloatTensor))\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "    return model, epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_accuracy_calculator(model, test_iterator):\n",
    "    \"\"\"\n",
    "    Function to canculate test data accuracy\n",
    "    \n",
    "    \"\"\"\n",
    "    epoch_acc = 0\n",
    "    for batch in test_iterator:\n",
    "        if batch.sentiment.shape[0] ==  32:\n",
    "            feature, target = batch.sentiment, batch.label\n",
    "            predictions = model(feature.to(device))            \n",
    "            acc = binary_accuracy(predictions.type(torch.FloatTensor), target.type(torch.FloatTensor))\n",
    "            epoch_acc += acc.item()\n",
    "    return  epoch_acc / len(test_iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining optimizer, loss and learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(EM.parameters(), lr=0.1,momentum=0.9)\n",
    "criterion = nn.MSELoss()\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the training for desired epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs  = 100\n",
    "log_interval = 1\n",
    "loss = []\n",
    "accuracy = []\n",
    "test_accuracy = []\n",
    "for i in tqdm(range(epochs)):\n",
    "    if (i != 0 and i%10 == 0 ):\n",
    "        # halving learning rate after every 10 epochs\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = param_group['lr']/2\n",
    "        print (\" === New Learning rate : \", param_group['lr'], \" === \")\n",
    "    \n",
    "    model, epoch_loss, epoch_acc = train(EM, train_iter, optimizer, criterion)\n",
    "    \n",
    "    test_acc = test_accuracy_calculator(model, test_iter)\n",
    "    accuracy.append(epoch_acc)\n",
    "    loss.append(epoch_loss)\n",
    "    test_accuracy.append(test_acc)\n",
    "    print(epoch_acc,test_acc,epoch_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Save ans load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initlizing model save and load object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SAL = saving_and_loading.objectManager()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAL.saver(EM, \"./EM.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EM = SAL.loader(\"./EM.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
