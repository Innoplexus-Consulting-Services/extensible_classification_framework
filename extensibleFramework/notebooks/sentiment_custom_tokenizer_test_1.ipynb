{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change Log\n",
    "---\n",
    "Integrating grid search with main code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/data/extensibleFramework/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/data/extensibleFramework/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from extensibleFramework.src.models import convolution_neural_network\n",
    "from extensibleFramework.src.models import recurrent_nn_with_attention\n",
    "from extensibleFramework.src.models import feed_forward_network\n",
    "from extensibleFramework.src.models import extra_layers\n",
    "from extensibleFramework.src.utils import saving_and_loading\n",
    "from extensibleFramework.src.utils import grid_search\n",
    "from extensibleFramework.src.models import config\n",
    "from extensibleFramework.src.utils import biomedical_tokenizer\n",
    "import torch\n",
    "import numpy as np\n",
    "from nltk.tokenize import MWETokenizer\n",
    "import json\n",
    "SAL = saving_and_loading.objectManager()\n",
    "tokenizer = biomedical_tokenizer.getToknizer()\n",
    "from nltk.tokenize import MWETokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import chakin\n",
    "import matplotlib.pyplot as plt\n",
    "from torchtext import data\n",
    "import nltk\n",
    "import json\n",
    "from torchtext import vocab\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import random\n",
    "import re\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import sys\n",
    "import random\n",
    "import tarfile\n",
    "import urllib\n",
    "from torchtext import data\n",
    "import datetime\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_frame = pd.read_csv(\"../data/crude/labeledTrainData.tsv\",sep=\"\\t\")\n",
    "data_frame = data_frame[['review', 'sentiment']]\n",
    "data_block  = data_frame.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SEED = 1\n",
    "split = 0.80\n",
    "batch_size  =32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# random.shuffle(data_block)\n",
    "\n",
    "# train_file = open('../data/processed/train.json', 'w')\n",
    "# test_file = open('../data/processed/test.json', 'w')\n",
    "# for i in  range(0,int(len(data_block)*split)):\n",
    "#     train_file.write(str(json.dumps({'review' : data_block[i][0], 'label' : data_block[i][1]}))+\"\\n\")\n",
    "# for i in  range(int(len(data_block)*split),len(data_block)):\n",
    "#     test_file.write(str(json.dumps({'review' : data_block[i][0], 'label' : data_block[i][1]}))+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tokenizer = MWETokenizer([],separator=' ')\n",
    "# for i in open(\"../src/resources/within_5.txt\").read().splitlines():\n",
    "#     tokenizer.add_mwe(i.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize(sentiments):\n",
    "    return tokenizer.tokenizer.tokenize(sentiments.split())\n",
    "def to_categorical(x):\n",
    "    x = int(x)\n",
    "    if x == 1:\n",
    "        return [0,1]\n",
    "    if x == 0:\n",
    "        return [1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'am', 'like', 'acne', 'Vulgaris', 'the', 'blah', 'sleep disorder', '.']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Tokenize test\n",
    "tokenize(\"i am like acne Vulgaris the blah sleep disorder .\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "REVIEW = data.Field(sequential=True , tokenize=tokenize, use_vocab = True, lower=True,batch_first=True)\n",
    "LABEL = data.Field(is_target=True,use_vocab = False, sequential=False, preprocessing = to_categorical)\n",
    "fields = {'review': ('review', REVIEW), 'label': ('label', LABEL)}\n",
    "train_data , test_data = data.TabularDataset.splits(\n",
    "                            path = '',\n",
    "                            train = '../data/processed/train.json',\n",
    "                            test = '../data/processed/test.json',\n",
    "                            format = 'json',\n",
    "                            fields = fields)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# chakin.search(lang='English')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# chakin.download(number=12, save_dir = \"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# !unzip glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vec = vocab.Vectors(name = \"../embedidngs/glove.6B.100d.txt\",cache = \"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "REVIEW.build_vocab(train_data, test_data, max_size=400000, vectors=vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentiment_vocab = REVIEW.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "116705"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentiment_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([116705, 100])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_vocab.vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class parameters:\n",
    "    def __init__(self):\n",
    "        self.cnn_rnn_vocab_size = len(sentiment_vocab)\n",
    "        self.cnn_rnn_embed_dim = 100\n",
    "        self.cnn_rnn_class_num = [50,100,200,400]\n",
    "        self.cnn_out_channel_num = [8,12, 24]\n",
    "        self.cnn_kernel_sizes =  [3,4,5]\n",
    "        self.rnn_n_layers = [1,2,3]\n",
    "        self.rnn_hidden_size =  [64, 128, 256]\n",
    "        self.use_pretrained_weights = True\n",
    "        self.cnn_rnn_weights =  sentiment_vocab.vectors\n",
    "        self.cnn_rnn_weight_is_trainable =   False\n",
    "        self.dropout = [0.2, 0.3 ,0.5]\n",
    "        self.batch_size = batch_size\n",
    "        self.merge_mode = \"CONCAT\"\n",
    "        self.ffn_activation =  \"Relu\"\n",
    "        self.ffn_final_output_classes =  2\n",
    "        self.ffn_perceptron_per_layer = [[50, 25], [100,50, 25],  [200,100,50, 25]]\n",
    "        self.ffn_layer_wise_dropout = 0.2\n",
    "        self.learning_rate =  [0.1,0.2]\n",
    "        self.momentum = [0.5,0.8,0.9]\n",
    "        self.device = device\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SP = grid_search.searchParameters(parameters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_iter, test_iter = data.Iterator.splits(\n",
    "        (train_data, test_data), sort_key=lambda x: len(x.review),\n",
    "        batch_sizes=(batch_size,batch_size), device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for batch in test_iter:\n",
    "    feature, target = batch.review, batch.label\n",
    "    print(feature.data.shape, target.data.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentiment_vocab.vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "class ensemble_model(nn.Module):\n",
    "    def __init__(self, config_object):\n",
    "        super(ensemble_model, self).__init__() \n",
    "        self.cnn = convolution_neural_network.cnn_text(config_object)\n",
    "        self.rnn_attention = recurrent_nn_with_attention.RNNAttentionModel(config_object)\n",
    "        self.merge_layer = extra_layers.MergeAndFlattern(config_object)\n",
    "        self.ffn = feed_forward_network.ffn(config_object)\n",
    "    def forward(self, x):\n",
    "        cnn_output = self.cnn(x)\n",
    "        rnnAttention_output = self.rnn_attention(x)\n",
    "        merge_layer_output = self.merge_layer(cnn_output,rnnAttention_output )\n",
    "        final_output = self.ffn(merge_layer_output)\n",
    "        \n",
    "#         print(\"cnn_output : \",cnn_output.shape)\n",
    "#         print(\"rnn_input : \",rnnAttention_output.shape)\n",
    "#         print(\"merge_layer_output : \", merge_layer_output.shape)\n",
    "#         print(\"final_output : \", final_output.shape)\n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for parma_set in SP.random_search(1):\n",
    "    configuration = config.Config(parma_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for parma_set in SP.random_search(1):\n",
    "#     configuration = config.Config(parma_set)\n",
    "#     EM  = ensemble_model(configuration)\n",
    "#     EM  = EM.to(device)\n",
    "#     print(json.dumps(configuration, default=lambda x: x.__dict__))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnn_input = torch.Tensor(np.random.randint(5, size=[8, 10])).long().to(device)\n",
    "rnn_input = torch.Tensor(np.random.random([8, 10])).long().to(device) #batch_size, input_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# EM(rnn_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "    rounded_preds = torch.argmax(preds, dim=1)\n",
    "    correct = (rounded_preds == torch.argmax(y, dim=1)).float() #convert into float for division \n",
    "    acc = correct.sum()/len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        feature, target = batch.review, batch.label\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(feature.to(device))            \n",
    "        loss = criterion(predictions.type(torch.FloatTensor), target.type(torch.FloatTensor))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        acc = binary_accuracy(predictions.type(torch.FloatTensor), target.type(torch.FloatTensor))\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "    return model, epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_accuracy_calculator(model, test_iterator):\n",
    "    epoch_acc = 0\n",
    "    for batch in test_iterator:\n",
    "        if batch.review.shape[0] ==  32:\n",
    "            feature, target = batch.review, batch.label\n",
    "            predictions = model(feature.to(device))            \n",
    "            acc = binary_accuracy(predictions.type(torch.FloatTensor), target.type(torch.FloatTensor))\n",
    "            epoch_acc += acc.item()\n",
    "    return  epoch_acc / len(test_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "epochs  = 100\n",
    "log_interval = 1\n",
    "loss = []\n",
    "accuracy = []\n",
    "test_accuracy = []\n",
    "for parma_set in SP.random_search(10):\n",
    "    configuration = config.Config(parma_set)\n",
    "    EM  = ensemble_model(configuration)\n",
    "    EM  = EM.to(device)\n",
    "    optimizer = torch.optim.SGD(EM.parameters(), lr=0.1,momentum=0.9)\n",
    "    criterion = nn.MSELoss()\n",
    "    criterion = criterion.to(device)\n",
    "    \n",
    "    for i in tqdm(range(epochs)):\n",
    "        if (i != 0 and i%10 == 0 ):\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = param_group['lr']/2\n",
    "            print(\" === New Learning rate : \", param_group['lr'], \" === \")\n",
    "\n",
    "        model, epoch_loss, epoch_acc = train(EM, train_iter, optimizer, criterion)\n",
    "\n",
    "        test_acc = test_accuracy_calculator(model, test_iter)\n",
    "        accuracy.append(epoch_acc)\n",
    "        loss.append(epoch_loss)\n",
    "        test_accuracy.append(test_acc)\n",
    "        print(epoch_acc,test_acc,epoch_loss)\n",
    "    del configuration.device\n",
    "    detailed_params  = json.dumps(configuration, default=lambda x: x.__dict__)\n",
    "    SAL.saver(EM, \"train\",model_performance_metrics={\"Train_accuracy\":epoch_acc,\"Test accuracy\": test_acc, \"Epoch Loss\":epoch_loss}, detailed_params=detailed_params )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# EM = SAL.loader(\"train/20190306-144431/ensemble_model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
