{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change Log\n",
    "---\n",
    "Testing model with Amazon data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/data/extensibleFramework/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from extensibleFramework.src.models import convolution_neural_network\n",
    "from extensibleFramework.src.models import recurrent_nn_with_attention\n",
    "from extensibleFramework.src.models import feed_forward_network\n",
    "from extensibleFramework.src.models import extra_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torchtext import data\n",
    "import nltk\n",
    "import json\n",
    "from torchtext import vocab\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import sys\n",
    "import random\n",
    "import tarfile\n",
    "import urllib\n",
    "from torchtext import data\n",
    "import datetime\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # DataSource : http://www.cs.cornell.edu/people/pabo/movie-review-data/\n",
    "# !wget http://www.cs.cornell.edu/people/pabo/movie-review-data/rt-polaritydata.tar.gz\n",
    "# !tar -xvf rt-polaritydata.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SEED = 1\n",
    "# split = 0.80\n",
    "# data_block = []\n",
    "# negative_data  = open('rt-polaritydata/rt-polarity.neg',encoding='utf8',errors='ignore').read().splitlines()\n",
    "# for i in negative_data:\n",
    "#         data_block.append({\"sentiment\":str(i.strip()),\"label\" : 0}) \n",
    "# positve_data  = open('rt-polaritydata/rt-polarity.pos',encoding='utf8',errors='ignore').read().splitlines()\n",
    "# for i in positve_data:\n",
    "#         data_block.append({\"sentiment\":str(i.strip()),\"label\" : 1}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# random.shuffle(data_block)\n",
    "\n",
    "# train_file = open('train.json', 'w')\n",
    "# test_file = open('test.json', 'w')\n",
    "# for i in  range(0,int(len(data_block)*split)):\n",
    "#     train_file.write(str(json.dumps(data_block[i]))+\"\\n\")\n",
    "# for i in  range(int(len(data_block)*split),len(data_block)):\n",
    "#     test_file.write(str(json.dumps(data_block[i]))+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize(sentiments):\n",
    "#     print(sentiments)\n",
    "    return sentiments\n",
    "def pad_to_equal(x):\n",
    "    if len(x) < 61:\n",
    "        return x + ['<pad>' for i in range(0, 61 - len(x))]\n",
    "    else:\n",
    "        return x[:61]\n",
    "def to_categorical(x):\n",
    "    if x == 1:\n",
    "        return [0,1]\n",
    "    if x == 0:\n",
    "        return [1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SENTIMENT = data.Field(sequential=True , preprocessing =pad_to_equal , use_vocab = True, lower=True,batch_first=True)\n",
    "LABEL = data.Field(is_target=True,use_vocab = False, sequential=False, preprocessing =to_categorical)\n",
    "fields = {'sentiment': ('sentiment', SENTIMENT), 'label': ('label', LABEL)}\n",
    "train_data , test_data = data.TabularDataset.splits(\n",
    "                            path = '',\n",
    "                            train = 'train.json',\n",
    "                            test = 'test.json',\n",
    "                            format = 'json',\n",
    "                            fields = fields)\n",
    "\n",
    "SENTIMENT.build_vocab(train_data, test_data)\n",
    "LABEL.build_vocab(train_data, test_data)\n",
    "train_iter, test_iter = data.Iterator.splits(\n",
    "        (train_data, test_data), sort_key=lambda x: len(x.sentiment),\n",
    "        batch_sizes=(16,len(test_data)), device=device,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 61]) torch.Size([16, 2])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_iter:\n",
    "    feature, target = batch.sentiment, batch.label\n",
    "    print(feature.data.shape, target.data.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "class ensemble_model(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, class_num, \\\n",
    "                 out_channel_num, kernel_size, dropout, input_size, hidden_size, batch_size, n_layers=1, rnn_unit=\"GRU\"):\n",
    "        super(ensemble_model, self).__init__() \n",
    "        self.cnn = convolution_neural_network.cnn_text(vocab_size, embed_dim, class_num, out_channel_num, kernel_size, dropout)\n",
    "        self.rnn_attention = recurrent_nn_with_attention.RNNAttentionModel(output_size=class_num, hidden_size=hidden_size,\\\n",
    "                                                                          vocab_size=vocab_size, embedding_length=embed_dim, \\\n",
    "                                                                          batch_size=batch_size)\n",
    "        self.merge_layer = extra_layers.MergeAndFlattern(mode=\"CONCAT\",batch_size=batch_size)\n",
    "        self.ffn = feed_forward_network.ffn(activation = \"Relu\", num_layer = 3 , input_size = 2000 , output_size = 2, perceptron_per_layer = [1000,500,250] , dropout = [0.2,0.2,0.2])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        cnn_output = self.cnn(x)\n",
    "        rnnAttention_output = self.rnn_attention(x)\n",
    "        merge_layer_output = self.merge_layer(cnn_output,rnnAttention_output )\n",
    "        final_output = self.ffn(merge_layer_output)\n",
    "        print(\"cnn_output : \",cnn_output.shape)\n",
    "        print(\"rnn_input : \",rnnAttention_output.shape)\n",
    "        print(\"merge_layer_output : \", merge_layer_output.shape)\n",
    "        print(\"final_output : \", final_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_size = 10\n",
    "hidden_size = 128\n",
    "batch_size = 8\n",
    "n_layers = 1\n",
    "vocab_size = 5\n",
    "embed_dim  = 100\n",
    "class_num =  1000\n",
    "out_channel_num = 8\n",
    "kernel_sizes = [2,3,4]\n",
    "dropout = 0.5\n",
    "\n",
    "EM  = ensemble_model(vocab_size=vocab_size, embed_dim=embed_dim, class_num=class_num,out_channel_num=out_channel_num,\\\n",
    "                    kernel_size=kernel_sizes, dropout=dropout, input_size = input_size, hidden_size=hidden_size,\\\n",
    "                    batch_size = batch_size, n_layers = n_layers, rnn_unit = \"GRU\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnn_input = torch.Tensor(np.random.randint(5, size=[8, 10])).long()\n",
    "rnn_input = torch.Tensor(np.random.random([8, 10])).long() #batch_size, input_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cnn_output :  torch.Size([8, 1000])\n",
      "rnn_input :  torch.Size([8, 1000])\n",
      "merge_layer_output :  torch.Size([8, 2000])\n",
      "final_output :  torch.Size([8, 2])\n"
     ]
    }
   ],
   "source": [
    "EM(cnn_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnn = CNN_Text( embed_num, embed_dim, class_num, kernel_num, kernel_sizes, dropout, static, stride)\n",
    "cnn = cnn.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "    rounded_preds = torch.argmax(preds, dim=1)\n",
    "    correct = (rounded_preds == torch.argmax(y, dim=1)).float() #convert into float for division \n",
    "    acc = correct.sum()/len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        feature, target = batch.sentiment, batch.label\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(feature)            \n",
    "        loss = criterion(predictions.type(torch.FloatTensor), target.type(torch.FloatTensor))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        acc = binary_accuracy(predictions.type(torch.FloatTensor), target.type(torch.FloatTensor))\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "    return model, epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_accuracy_calculator(model, test_iterator):\n",
    "    epoch_acc = 0\n",
    "    for batch in test_iterator:\n",
    "        feature, target = batch.sentiment, batch.label\n",
    "        predictions = model(feature)            \n",
    "        acc = binary_accuracy(predictions.type(torch.FloatTensor), target.type(torch.FloatTensor))\n",
    "        epoch_acc += acc.item()\n",
    "    return  epoch_acc / len(test_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(cnn.parameters(), lr=0.0001 , weight_decay=0.0001)\n",
    "criterion = nn.MSELoss()\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs  = 100\n",
    "log_interval = 1\n",
    "loss = []\n",
    "accuracy = []\n",
    "test_accuracy = []\n",
    "for i in tqdm(range(epochs)):\n",
    "    model, epoch_loss, epoch_acc = train(cnn, train_iter, optimizer, criterion)\n",
    "    test_acc = test_accuracy_calculator(model, test_iter)\n",
    "    accuracy.append(epoch_acc)\n",
    "    loss.append(epoch_loss)\n",
    "    test_accuracy.append(test_acc)\n",
    "    print(epoch_acc,test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
